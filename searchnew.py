import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import time
from io import BytesIO
from docx import Document
from docx.shared import Inches

# æ¤œç´¢ã—ã¦URLã‚’å–å¾—ã™ã‚‹é–¢æ•°
def search_and_scrape(search_query):
    search_query = search_query.strip().replace(' ', '%20')
    result_links = []
    page_num = 1
    pattern = re.compile(r'/([1-9][0-9]{2,})[A-Za-z]\d{2}')

    while page_num <= 6:
        if page_num == 1:
            url = f'https://medu4.com/quizzes/result?q={search_query}&st=all'
        else:
            url = f'https://medu4.com/quizzes/result?page={page_num}&q={search_query}&st=all'

        response = requests.get(url)
        if response.status_code != 200:
            break

        soup = BeautifulSoup(response.text, 'html.parser')
        all_links = [link['href'] for link in soup.find_all('a', href=True)]
        page_results = [link for link in all_links if pattern.search(link)]

        if not page_results:
            break

        result_links.extend(page_results)
        page_num += 1
        time.sleep(0.5)

    full_urls = [f"https://medu4.com{link}" for link in result_links]
    return full_urls

# ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—ï¼ˆè¤‡æ•°ç”»åƒå¯¾å¿œï¼‰
def get_page_text(url, get_images=True):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    category = soup.find('span', class_='button-small-line')
    category_name = category.text.strip() if category else 'åˆ†é‡åãªã—'

    problem = soup.find('div', class_='quiz-body mb-64')
    problem_text = problem.text.strip() if problem else 'å•é¡Œæ–‡ãªã—'

    choices = []
    for choice in soup.find_all('div', class_='box-select'):
        choice_header = choice.find('span', {'class': 'choice-header'}).text.strip()
        choice_text = choice.find_all('span')[1].text.strip()
        choices.append(f"{choice_header} {choice_text}")

    h4_tags = soup.find_all('h4')
    answer_text = 'è§£ç­”ãªã—'
    question_id = 'å•é¡Œç•ªå·ãªã—'
    if len(h4_tags) >= 2:
        answer_text = h4_tags[0].text.strip()
        question_id_match = re.search(r'([0-9]{3}[A-Za-z][0-9]+)', h4_tags[1].text)
        if question_id_match:
            question_id = question_id_match.group(1)

    explanation = soup.find('div', class_='explanation')
    explanation_text = explanation.text.strip() if explanation else 'è§£èª¬ãªã—'

    image_urls = []
    if get_images:
        image_divs = soup.find_all('div', class_='box-quiz-image mb-32')
        for div in image_divs:
            a_tags = div.find_all('a')
            for a_tag in a_tags:
                href = a_tag.get('href')
                if href and href.endswith('.jpg'):
                    full_image_url = href.replace('thumb_', '')
                    image_urls.append(full_image_url)

    return {
        "category": category_name,
        "problem": problem_text,
        "choices": choices,
        "answer": answer_text,
        "question_id": question_id,
        "explanation": explanation_text,
        "images": image_urls
    }

# Wordãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆç”»åƒåŸ‹ã‚è¾¼ã¿å¯¾å¿œï¼‰
def create_word_doc(pages_data, search_query, include_images=True):
    doc = Document()
    doc.add_heading('æ¤œç´¢çµæœ', 0)
    doc.add_paragraph(f"å–å¾—å•é¡Œæ•°: {len(pages_data)}å•")

    for idx, page_data in enumerate(pages_data, start=1):
        title = f"å•é¡Œ{idx} {page_data['question_id']}"
        doc.add_paragraph(title, style='Heading2')
        doc.add_paragraph(f"å•é¡Œæ–‡: {page_data['problem']}")

        if include_images and page_data['images']:
            for img_url in page_data['images']:
                try:
                    response = requests.get(img_url)
                    if response.status_code == 200:
                        image_stream = BytesIO(response.content)
                        doc.add_paragraph()
                        doc.add_picture(image_stream, width=Inches(2.5))
                    else:
                        doc.add_paragraph(f"ç”»åƒå–å¾—å¤±æ•—: {img_url}")
                except Exception as e:
                    doc.add_paragraph(f"ç”»åƒå–å¾—ä¸­ã‚¨ãƒ©ãƒ¼: {e}")

        doc.add_paragraph("é¸æŠè‚¢:")
        for choice in page_data['choices']:
            doc.add_paragraph(choice)
        doc.add_paragraph(f"{page_data['answer']}")
        doc.add_paragraph(f"è§£èª¬: {page_data['explanation']}")
        doc.add_paragraph("-" * 50)

    filename = f"{search_query}_search_results.docx"
    doc.save(filename)
    return filename

# Streamlit UI
st.title("Medu4 æ¤œç´¢ãƒ„ãƒ¼ãƒ«New3")
search_query = st.text_input("æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

col1, col2 = st.columns(2)

def run_search(get_images: bool):
    with st.spinner("æ¤œç´¢ä¸­..."):
        result_pages = search_and_scrape(search_query)

    if result_pages:
        st.write(f"{len(result_pages)}ä»¶ã®å•é¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
        progress_bar = st.progress(0)
        status_text = st.empty()

        pages_data = []
        for i, url in enumerate(result_pages):
            page_data = get_page_text(url, get_images=get_images)
            pages_data.append(page_data)

            progress = int((i + 1) / len(result_pages) * 100)
            progress_bar.progress(progress)
            status_text.text(f"{i + 1} / {len(result_pages)} ä»¶å–å¾—ä¸­...")

        with st.spinner("Wordãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­..."):
            filename = create_word_doc(pages_data, search_query, include_images=get_images)

        st.success("Wordãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Œæˆã—ã¾ã—ãŸï¼")
        with open(filename, "rb") as file:
            st.download_button("ğŸ“„ Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", file, file_name=filename)
    else:
        st.error("æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

with col1:
    if st.button("ğŸ” æ¤œç´¢ï¼ˆç”»åƒã‚ã‚Šï¼‰"):
        run_search(get_images=True)

with col2:
    if st.button("âš¡ æ¤œç´¢ï¼ˆç”»åƒãªã—ï¼‰"):
        run_search(get_images=False)
